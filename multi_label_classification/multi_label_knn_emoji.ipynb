{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tslearn.metrics import dtw, soft_dtw\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from sklearn.utils.validation import _check_large_sparse\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "from sklearn.metrics import pairwise_distances_argmin_min, jaccard_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_df(item_lists, unique_items):\n",
    "# Create empty dict\n",
    "    bool_dict = {}\n",
    "    \n",
    "    # Loop through all the tags\n",
    "    for i, item in enumerate(unique_items):\n",
    "        \n",
    "        # Apply boolean mask\n",
    "        bool_dict[item] = item_lists.apply(lambda x: 1 if item in x else 0)\n",
    "            \n",
    "    # Return the results as a dataframe\n",
    "    return pd.DataFrame(bool_dict)\n",
    "\n",
    "def to_1D(series):\n",
    "    return pd.Series([x for _list in series for x in _list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_emoji_map = {'neutral': 'none', 'smirk':'contempt', 'furious':'furious', 'weary':'annoyed', 'expressionless':'none', 'unamused': 'annoyed', 'rollingeyes':'contempt', 'none':'none', 'skeptical':'none', 'angry':'anger', 'nauseated':'disgust', 'vomiting':'disgust', 'triumph':'anger', 'hatred':'hatred'}\n",
    "na_emoji_map = {'neutral': ['none'], 'smirk':['contempt'], 'furious':['furious', 'anger'], 'weary':['disgust'], 'expressionless':['contempt','annoyed'], 'unamused': ['annoyed', 'contempt'], 'rollingeyes':['annoyed'], 'none':['none'], 'skeptical':['none'], 'angry':['anger', 'annoyed'], 'nauseated':['disgust'], 'vomiting':['disgust'], 'triumph':['anger'], 'hatred':['hatred', 'furious']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.read_csv('../new_data/NA/na_dataset.csv', index_col=None)\n",
    "Y_df = pd.read_csv('../new_data/NA/na_emoji_labels.csv', usecols=['filename', 'emoji'], index_col='filename')\n",
    "emotion_df = pd.read_csv('../new_data/NA/na_labels.csv', usecols=['filename', 'emotions'], index_col='filename')\n",
    "\n",
    "Y_df[\"emoji\"] = Y_df[\"emoji\"].apply(eval)\n",
    "emotion_df[\"emotions\"] = emotion_df[\"emotions\"].apply(eval)\n",
    "label_cols = to_1D(Y_df[\"emoji\"]).unique() \n",
    "emotion_cols = to_1D(emotion_df[\"emotions\"]).unique() \n",
    "# label_cols.append('smilingimp')\n",
    "labels_expanded = boolean_df(Y_df['emoji'], label_cols )\n",
    "emotions_expanded = boolean_df(emotion_df['emotions'], emotion_cols )\n",
    "\n",
    "order = [2, 11, 9, 13, 10, 12, 3, 4, 5, 6, 8, 7, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['none', 'furious', 'anger', 'annoyed', 'contempt', 'disgust',\n",
       "       'hatred'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "emotion_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_expanded.loc['na/vid_5.mp4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in label_cols:\n",
    "    X_df[label_cols]  = np.NaN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_expanded.at['na/vid_5.mp4', 'hatred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in X_df.iterrows():\n",
    "    # print(index, row)\n",
    "    filename = X_df.iloc[index]['filename']\n",
    "    # print(filename)\n",
    "    for l in label_cols.tolist():\n",
    "        try:\n",
    "    # print(labels_expanded.loc[filename]['none':'hatred'].to_list())\n",
    "            X_df.at[index, l] = labels_expanded.at[filename, l]\n",
    "        except Exception:\n",
    "            # print(\"here\")\n",
    "            print('expanded labels: filename: {}, label: {}, {}'.format(filename, l, labels_expanded.at[filename, l]))\n",
    "            print(X_df.at[index, l])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in X_df.iterrows():\n",
    "    # print(index, row)\n",
    "    filename = X_df.iloc[index]['filename']\n",
    "    # print(filename)\n",
    "    for l in emotion_cols.tolist():\n",
    "        try:\n",
    "    # print(labels_expanded.loc[filename]['none':'hatred'].to_list())\n",
    "            X_df.at[index, l] = emotions_expanded.at[filename, l]\n",
    "        except Exception:\n",
    "            # print(\"here\")\n",
    "            print('expanded labels: filename: {}, label: {}, {}'.format(filename, l, emotions_expanded.at[filename, l]))\n",
    "            print(X_df.at[index, l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       filename         culture  frame  face_id  timestamp  confidence  \\\n",
       "0  na/vid_1.mp4  north american      1        0      0.000        0.98   \n",
       "1  na/vid_1.mp4  north american      2        0      0.017        0.98   \n",
       "2  na/vid_1.mp4  north american      3        0      0.033        0.98   \n",
       "3  na/vid_1.mp4  north american      4        0      0.050        0.98   \n",
       "4  na/vid_1.mp4  north american      5        0      0.067        0.98   \n",
       "\n",
       "   success  AU01_r  AU02_r  AU04_r  ...  skeptical  angry  nauseated  \\\n",
       "0        1    1.45    1.86     0.0  ...        0.0    0.0        0.0   \n",
       "1        1    1.50    1.98     0.0  ...        0.0    0.0        0.0   \n",
       "2        1    1.57    1.98     0.0  ...        0.0    0.0        0.0   \n",
       "3        1    1.56    1.99     0.0  ...        0.0    0.0        0.0   \n",
       "4        1    1.40    1.94     0.0  ...        0.0    0.0        0.0   \n",
       "\n",
       "   vomiting  triumph  hatred  anger  annoyed  contempt  disgust  \n",
       "0       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n",
       "1       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n",
       "2       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n",
       "3       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n",
       "4       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>culture</th>\n      <th>frame</th>\n      <th>face_id</th>\n      <th>timestamp</th>\n      <th>confidence</th>\n      <th>success</th>\n      <th>AU01_r</th>\n      <th>AU02_r</th>\n      <th>AU04_r</th>\n      <th>...</th>\n      <th>skeptical</th>\n      <th>angry</th>\n      <th>nauseated</th>\n      <th>vomiting</th>\n      <th>triumph</th>\n      <th>hatred</th>\n      <th>anger</th>\n      <th>annoyed</th>\n      <th>contempt</th>\n      <th>disgust</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>na/vid_1.mp4</td>\n      <td>north american</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000</td>\n      <td>0.98</td>\n      <td>1</td>\n      <td>1.45</td>\n      <td>1.86</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>na/vid_1.mp4</td>\n      <td>north american</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.017</td>\n      <td>0.98</td>\n      <td>1</td>\n      <td>1.50</td>\n      <td>1.98</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>na/vid_1.mp4</td>\n      <td>north american</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0.033</td>\n      <td>0.98</td>\n      <td>1</td>\n      <td>1.57</td>\n      <td>1.98</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>na/vid_1.mp4</td>\n      <td>north american</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0.050</td>\n      <td>0.98</td>\n      <td>1</td>\n      <td>1.56</td>\n      <td>1.99</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>na/vid_1.mp4</td>\n      <td>north american</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0.067</td>\n      <td>0.98</td>\n      <td>1</td>\n      <td>1.40</td>\n      <td>1.94</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 47 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "source": [
    "### Min-Max Scaling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ablation cols\n",
    "ablation_cols = ['AU01_r','AU02_r','AU04_r','AU05_r','AU06_r','AU07_r','AU09_r', 'AU10_r','AU12_r','AU14_r','AU15_r','AU17_r','AU20_r','AU23_r','AU25_r','AU26_r','AU45_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = X_df.drop(columns=ablation_cols)\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = list (\n",
    "    set(X_df.columns.to_list()) - set(['frame', 'face_id', 'culture', 'filename', 'timestamp', 'confidence','success'])- set(label_cols)-set(emotion_cols)\n",
    "    )\n",
    "scaler = MinMaxScaler()\n",
    "X_df[cols_to_scale] = scaler.fit_transform(X_df[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting into train and test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['frame', 'face_id', 'culture', 'filename', 'timestamp', 'confidence','success'] + label_cols.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       filename         culture  frame  face_id  timestamp  confidence  \\\n0  na/vid_1.mp4  north american      1        0      0.000        0.98   \n1  na/vid_1.mp4  north american      2        0      0.017        0.98   \n2  na/vid_1.mp4  north american      3        0      0.033        0.98   \n3  na/vid_1.mp4  north american      4        0      0.050        0.98   \n4  na/vid_1.mp4  north american      5        0      0.067        0.98   \n\n   success    AU01_r  AU02_r  AU04_r  ...  skeptical  angry  nauseated  \\\n0        1  0.386667   0.372     0.0  ...        0.0    0.0        0.0   \n1        1  0.400000   0.396     0.0  ...        0.0    0.0        0.0   \n2        1  0.418667   0.396     0.0  ...        0.0    0.0        0.0   \n3        1  0.416000   0.398     0.0  ...        0.0    0.0        0.0   \n4        1  0.373333   0.388     0.0  ...        0.0    0.0        0.0   \n\n   vomiting  triumph  hatred  anger  annoyed  contempt  disgust  \n0       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n1       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n2       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n3       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n4       0.0      0.0     0.0    0.0      0.0       0.0      0.0  \n\n[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "metadata_cols = ['frame', 'face_id', 'culture', 'filename', 'timestamp']\n",
    "cols_to_drop = ['frame', 'face_id', 'culture', 'filename', 'timestamp', 'confidence','success'] + label_cols.tolist() + emotion_cols.tolist()\n",
    "print(X_df.head())\n",
    "videos = X_df['filename'].unique()\n",
    "# test_videos = pd.Series(videos).sample(frac=0.35)\n",
    "test_videos = ['na/vid_52.mp4', 'na/vid_13.mp4', 'na/vid_54.mp4', 'na/vid_92.mp4', 'na/vid_48.mp4', 'na/vid_93.mp4', 'na/vid_6.mp4', 'na/vid_50.mp4', 'na/vid_14.mp4', 'na/vid_10_1.mp4', 'na/vid_34.mp4', 'na/vid_86.mp4', 'na/vid_83.mp4', 'na/vid_90.mp4', 'na/vid_55.mp4', 'na/vid_60.mp4', 'na/vid_24.mp4', 'na/vid_10_3.mp4', 'na/vid_10_2.mp4', 'na/vid_87.mp4', 'na/vid_32.mp4', 'na/vid_79.mp4', 'na/vid_68.mp4', 'na/vid_56.mp4']\n",
    "# test_videos = ['persian/vid_52.mp4', 'persian/vid_40.mp4', 'persian/vid_66.mp4', 'persian/vid_18.mp4', 'persian/vid_96.mp4', 'persian/vid_65.mp4', 'persian/vid_51.mp4', 'persian/vid_85.mp4', 'persian/vid_87.mp4', 'persian/vid_70.mp4', 'persian/vid_76.mp4', 'persian/vid_27.mp4', 'persian/vid_88.mp4', 'persian/vid_21.mp4', 'persian/vid_4.mp4', 'persian/vid_61.mp4', 'persian/vid_12.mp4', 'persian/vid_46.mp4', 'persian/vid_81.mp4', 'persian/vid_93.mp4', 'persian/vid_56.mp4', 'persian/vid_55.mp4', 'persian/vid_68.mp4', 'persian/vid_38.mp4', 'persian/vid_62.mp4', 'persian/vid_14.mp4', 'persian/vid_8.mp4', 'persian/vid_83.mp4', 'persian/vid_45.mp4', 'persian/vid_91.mp4', 'persian/vid_22.mp4', 'persian/vid_36.mp4', 'persian/vid_7.mp4']\n",
    "train_videos = np.array(list(set(videos) - set(test_videos)))\n",
    "test_df = X_df[X_df['filename'].isin(test_videos)]\n",
    "metadata_test = test_df[metadata_cols]\n",
    "y_test = test_df[label_cols].values\n",
    "y_test_emotions = test_df[emotion_cols]\n",
    "X_test = test_df.drop(columns = cols_to_drop).values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_videos.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_test.iloc[800:805]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Cross-validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1-th split: train: 56, test: 14\n",
      "['na/vid_15.mp4' 'na/vid_44.mp4' 'na/vid_69.mp4' 'na/vid_26.mp4'\n",
      " 'na/vid_38.mp4' 'na/vid_21.mp4' 'na/vid_33.mp4' 'na/vid_1.mp4'\n",
      " 'na/vid_20.mp4' 'na/vid_74.mp4' 'na/vid_43.mp4' 'na/vid_51.mp4'\n",
      " 'na/vid_31.mp4' 'na/vid_46.mp4' 'na/vid_95.mp4' 'na/vid_62.mp4'\n",
      " 'na/vid_11.mp4' 'na/vid_75.mp4' 'na/vid_67.mp4' 'na/vid_81.mp4'\n",
      " 'na/vid_104.mp4' 'na/vid_84.mp4' 'na/vid_45.mp4' 'na/vid_23.mp4'\n",
      " 'na/vid_25.mp4' 'na/vid_4.mp4' 'na/vid_85.mp4' 'na/vid_77.mp4'\n",
      " 'na/vid_80.mp4' 'na/vid_78.mp4' 'na/vid_53.mp4' 'na/vid_22.mp4'\n",
      " 'na/vid_65.mp4' 'na/vid_101.mp4' 'na/vid_76.mp4' 'na/vid_63.mp4'\n",
      " 'na/vid_27.mp4' 'na/vid_47.mp4' 'na/vid_5.mp4' 'na/vid_29.mp4'\n",
      " 'na/vid_2.mp4' 'na/vid_70.mp4' 'na/vid_16.mp4' 'na/vid_30.mp4'\n",
      " 'na/vid_42.mp4' 'na/vid_88.mp4' 'na/vid_100.mp4' 'na/vid_17.mp4'\n",
      " 'na/vid_18.mp4' 'na/vid_19.mp4' 'na/vid_73.mp4' 'na/vid_8.mp4'\n",
      " 'na/vid_66.mp4' 'na/vid_7.mp4' 'na/vid_3.mp4' 'na/vid_12.mp4']\n",
      "Training+validation data size:  4196\n",
      "Training data size:  4196\n",
      "Validation data size:  1090\n",
      "XGB-multi validation Jaccard score: 0.08634556574923546\n",
      "XGB-multi validation Hamming loss:  0.12778505897771952\n",
      "XGB-multi test Jaccard score:  0.16231281820904464\n",
      "XGB-multi test Hamming loss:  0.13522012578616352\n",
      "CC Validation Jaccard Score:\n",
      "  0.11480122324159021\n",
      "CC Validation Hamming Loss:\n",
      "  0.13171690694626476\n",
      "CC Test Jaccard Score: \n",
      "  0.1639602675451732\n",
      "CC Test Hamming Loss:\n",
      "  0.13634321653189577\n",
      "(24, 14)\n",
      "(24, 14)\n",
      "2-th split: train: 56, test: 14\n",
      "['na/vid_15.mp4' 'na/vid_44.mp4' 'na/vid_72.mp4' 'na/vid_26.mp4'\n",
      " 'na/vid_38.mp4' 'na/vid_21.mp4' 'na/vid_33.mp4' 'na/vid_1.mp4'\n",
      " 'na/vid_20.mp4' 'na/vid_74.mp4' 'na/vid_43.mp4' 'na/vid_51.mp4'\n",
      " 'na/vid_31.mp4' 'na/vid_46.mp4' 'na/vid_95.mp4' 'na/vid_62.mp4'\n",
      " 'na/vid_11.mp4' 'na/vid_75.mp4' 'na/vid_67.mp4' 'na/vid_81.mp4'\n",
      " 'na/vid_84.mp4' 'na/vid_45.mp4' 'na/vid_23.mp4' 'na/vid_25.mp4'\n",
      " 'na/vid_59.mp4' 'na/vid_57.mp4' 'na/vid_4.mp4' 'na/vid_85.mp4'\n",
      " 'na/vid_77.mp4' 'na/vid_49.mp4' 'na/vid_80.mp4' 'na/vid_82.mp4'\n",
      " 'na/vid_35.mp4' 'na/vid_22.mp4' 'na/vid_97.mp4' 'na/vid_101.mp4'\n",
      " 'na/vid_76.mp4' 'na/vid_63.mp4' 'na/vid_47.mp4' 'na/vid_5.mp4'\n",
      " 'na/vid_29.mp4' 'na/vid_89.mp4' 'na/vid_61.mp4' 'na/vid_70.mp4'\n",
      " 'na/vid_37.mp4' 'na/vid_58.mp4' 'na/vid_88.mp4' 'na/vid_36.mp4'\n",
      " 'na/vid_19.mp4' 'na/vid_8.mp4' 'na/vid_66.mp4' 'na/vid_39.mp4'\n",
      " 'na/vid_9.mp4' 'na/vid_7.mp4' 'na/vid_3.mp4' 'na/vid_12.mp4']\n",
      "Training+validation data size:  4195\n",
      "Training data size:  4195\n",
      "Validation data size:  1091\n",
      "XGB-multi validation Jaccard score: 0.18087381607088296\n",
      "XGB-multi validation Hamming loss:  0.11535943433285321\n",
      "XGB-multi test Jaccard score:  0.10289383048817012\n",
      "XGB-multi test Hamming loss:  0.14236298292902066\n",
      "CC Validation Jaccard Score:\n",
      "  0.25109990834097157\n",
      "CC Validation Hamming Loss:\n",
      "  0.1122168390729344\n",
      "CC Test Jaccard Score: \n",
      "  0.12493585903963263\n",
      "CC Test Hamming Loss:\n",
      "  0.1398023360287511\n",
      "(24, 14)\n",
      "(24, 14)\n",
      "3-th split: train: 56, test: 14\n",
      "['na/vid_44.mp4' 'na/vid_72.mp4' 'na/vid_69.mp4' 'na/vid_26.mp4'\n",
      " 'na/vid_38.mp4' 'na/vid_21.mp4' 'na/vid_33.mp4' 'na/vid_20.mp4'\n",
      " 'na/vid_43.mp4' 'na/vid_51.mp4' 'na/vid_46.mp4' 'na/vid_62.mp4'\n",
      " 'na/vid_75.mp4' 'na/vid_81.mp4' 'na/vid_104.mp4' 'na/vid_45.mp4'\n",
      " 'na/vid_25.mp4' 'na/vid_59.mp4' 'na/vid_57.mp4' 'na/vid_4.mp4'\n",
      " 'na/vid_85.mp4' 'na/vid_77.mp4' 'na/vid_49.mp4' 'na/vid_80.mp4'\n",
      " 'na/vid_78.mp4' 'na/vid_53.mp4' 'na/vid_82.mp4' 'na/vid_35.mp4'\n",
      " 'na/vid_22.mp4' 'na/vid_65.mp4' 'na/vid_97.mp4' 'na/vid_76.mp4'\n",
      " 'na/vid_63.mp4' 'na/vid_27.mp4' 'na/vid_5.mp4' 'na/vid_29.mp4'\n",
      " 'na/vid_2.mp4' 'na/vid_89.mp4' 'na/vid_61.mp4' 'na/vid_70.mp4'\n",
      " 'na/vid_16.mp4' 'na/vid_30.mp4' 'na/vid_37.mp4' 'na/vid_58.mp4'\n",
      " 'na/vid_42.mp4' 'na/vid_88.mp4' 'na/vid_100.mp4' 'na/vid_17.mp4'\n",
      " 'na/vid_18.mp4' 'na/vid_36.mp4' 'na/vid_73.mp4' 'na/vid_66.mp4'\n",
      " 'na/vid_39.mp4' 'na/vid_9.mp4' 'na/vid_3.mp4' 'na/vid_12.mp4']\n",
      "Training+validation data size:  4146\n",
      "Training data size:  4146\n",
      "Validation data size:  1140\n",
      "XGB-multi validation Jaccard score: 0.12054093567251462\n",
      "XGB-multi validation Hamming loss:  0.13784461152882205\n",
      "XGB-multi test Jaccard score:  0.13456124588200058\n",
      "XGB-multi test Hamming loss:  0.13562443845462713\n",
      "CC Validation Jaccard Score:\n",
      "  0.1361842105263158\n",
      "CC Validation Hamming Loss:\n",
      "  0.1400375939849624\n",
      "CC Test Jaccard Score: \n",
      "  0.14706498951781968\n",
      "CC Test Hamming Loss:\n",
      "  0.13656783468104222\n",
      "(24, 14)\n",
      "(24, 14)\n",
      "4-th split: train: 56, test: 14\n",
      "['na/vid_15.mp4' 'na/vid_44.mp4' 'na/vid_72.mp4' 'na/vid_69.mp4'\n",
      " 'na/vid_38.mp4' 'na/vid_21.mp4' 'na/vid_33.mp4' 'na/vid_1.mp4'\n",
      " 'na/vid_20.mp4' 'na/vid_74.mp4' 'na/vid_51.mp4' 'na/vid_31.mp4'\n",
      " 'na/vid_95.mp4' 'na/vid_62.mp4' 'na/vid_11.mp4' 'na/vid_67.mp4'\n",
      " 'na/vid_81.mp4' 'na/vid_104.mp4' 'na/vid_84.mp4' 'na/vid_23.mp4'\n",
      " 'na/vid_25.mp4' 'na/vid_59.mp4' 'na/vid_57.mp4' 'na/vid_49.mp4'\n",
      " 'na/vid_78.mp4' 'na/vid_53.mp4' 'na/vid_82.mp4' 'na/vid_35.mp4'\n",
      " 'na/vid_22.mp4' 'na/vid_65.mp4' 'na/vid_97.mp4' 'na/vid_101.mp4'\n",
      " 'na/vid_27.mp4' 'na/vid_47.mp4' 'na/vid_5.mp4' 'na/vid_2.mp4'\n",
      " 'na/vid_89.mp4' 'na/vid_61.mp4' 'na/vid_70.mp4' 'na/vid_16.mp4'\n",
      " 'na/vid_30.mp4' 'na/vid_37.mp4' 'na/vid_58.mp4' 'na/vid_42.mp4'\n",
      " 'na/vid_100.mp4' 'na/vid_17.mp4' 'na/vid_18.mp4' 'na/vid_36.mp4'\n",
      " 'na/vid_19.mp4' 'na/vid_73.mp4' 'na/vid_8.mp4' 'na/vid_66.mp4'\n",
      " 'na/vid_39.mp4' 'na/vid_9.mp4' 'na/vid_7.mp4' 'na/vid_3.mp4']\n",
      "Training+validation data size:  4273\n",
      "Training data size:  4273\n",
      "Validation data size:  1013\n",
      "XGB-multi validation Jaccard score: 0.1261928265876933\n",
      "XGB-multi validation Hamming loss:  0.13933154703144832\n",
      "XGB-multi test Jaccard score:  0.15077493261455524\n",
      "XGB-multi test Hamming loss:  0.1386343216531896\n",
      "CC Validation Jaccard Score:\n",
      "  0.14198749588680487\n",
      "CC Validation Hamming Loss:\n",
      "  0.13742772528557326\n",
      "CC Test Jaccard Score: \n",
      "  0.155651642208246\n",
      "CC Test Hamming Loss:\n",
      "  0.13589398023360288\n",
      "(24, 14)\n",
      "(24, 14)\n",
      "5-th split: train: 56, test: 14\n",
      "['na/vid_15.mp4' 'na/vid_72.mp4' 'na/vid_69.mp4' 'na/vid_26.mp4'\n",
      " 'na/vid_1.mp4' 'na/vid_74.mp4' 'na/vid_43.mp4' 'na/vid_31.mp4'\n",
      " 'na/vid_46.mp4' 'na/vid_95.mp4' 'na/vid_11.mp4' 'na/vid_75.mp4'\n",
      " 'na/vid_67.mp4' 'na/vid_104.mp4' 'na/vid_84.mp4' 'na/vid_45.mp4'\n",
      " 'na/vid_23.mp4' 'na/vid_59.mp4' 'na/vid_57.mp4' 'na/vid_4.mp4'\n",
      " 'na/vid_85.mp4' 'na/vid_77.mp4' 'na/vid_49.mp4' 'na/vid_80.mp4'\n",
      " 'na/vid_78.mp4' 'na/vid_53.mp4' 'na/vid_82.mp4' 'na/vid_35.mp4'\n",
      " 'na/vid_65.mp4' 'na/vid_97.mp4' 'na/vid_101.mp4' 'na/vid_76.mp4'\n",
      " 'na/vid_63.mp4' 'na/vid_27.mp4' 'na/vid_47.mp4' 'na/vid_29.mp4'\n",
      " 'na/vid_2.mp4' 'na/vid_89.mp4' 'na/vid_61.mp4' 'na/vid_16.mp4'\n",
      " 'na/vid_30.mp4' 'na/vid_37.mp4' 'na/vid_58.mp4' 'na/vid_42.mp4'\n",
      " 'na/vid_88.mp4' 'na/vid_100.mp4' 'na/vid_17.mp4' 'na/vid_18.mp4'\n",
      " 'na/vid_36.mp4' 'na/vid_19.mp4' 'na/vid_73.mp4' 'na/vid_8.mp4'\n",
      " 'na/vid_39.mp4' 'na/vid_9.mp4' 'na/vid_7.mp4' 'na/vid_12.mp4']\n",
      "Training+validation data size:  4334\n",
      "Training data size:  4334\n",
      "Validation data size:  952\n",
      "XGB-multi validation Jaccard score: 0.21999299719887955\n",
      "XGB-multi validation Hamming loss:  0.140531212484994\n",
      "XGB-multi test Jaccard score:  0.14464660077867625\n",
      "XGB-multi test Hamming loss:  0.12960467205750226\n",
      "CC Validation Jaccard Score:\n",
      "  0.253623949579832\n",
      "CC Validation Hamming Loss:\n",
      "  0.1400060024009604\n",
      "CC Test Jaccard Score: \n",
      "  0.14553009883198562\n",
      "CC Test Hamming Loss:\n",
      "  0.1274483378256963\n",
      "(24, 14)\n",
      "(24, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "kfold = KFold(5, True, 1)\n",
    "frames_mean_hm_test = []\n",
    "frames_mean_jac_test = []\n",
    "videos_mean_jac_test = []\n",
    "videos_mean_hm_test = []\n",
    "# metadata_test.reset_index(inplace=True)\n",
    "order = [2, 11, 9, 13, 10, 12, 3, 4, 5, 6, 8, 7, 0]\n",
    "col_indices = {i:label for (i,label) in enumerate(label_cols)}\n",
    "\n",
    "splits = kfold.split(train_videos)\n",
    "for (i, (train, test)) in enumerate(splits):\n",
    "    # print(videos[train])\n",
    "    # print(videos[test])\n",
    "    print('%d-th split: train: %d, test: %d' % (i+1, len(train_videos[train]), len(train_videos[test])))\n",
    "    train_df = X_df[X_df['filename'].isin(train_videos[train])]\n",
    "    print(train_videos[train])\n",
    "    train_metadata = train_df[metadata_cols]\n",
    "    print('Training+validation data size: ', train_df.shape[0])\n",
    "    y_train = train_df[label_cols].values\n",
    "    X_train = train_df.drop(columns = cols_to_drop).values\n",
    "    valid_df = X_df[X_df['filename'].isin(train_videos[test])]\n",
    "    y_valid = valid_df[label_cols].values\n",
    "    X_valid = valid_df.drop(columns = cols_to_drop).values\n",
    "    # X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "    print('Training data size: ', X_train.shape[0])\n",
    "    print('Validation data size: ', X_valid.shape[0])\n",
    "    base_knn =  KNeighborsClassifier(n_neighbors=5,)\n",
    "    base_lr = LogisticRegression()\n",
    "    base_rf = RandomForestClassifier()\n",
    "    base_xgb = XGBClassifier(objective=\"binary:logistic\", eval_metric='logloss')\n",
    "\n",
    "    multiclass_xg = MultiOutputClassifier(XGBClassifier(objective='binary:logistic', eval_metric='auc'))\n",
    "    # ovr = LogisticRegression()\n",
    "    multiclass_xg.fit(X_train, y_train)\n",
    "    valid_pred_xgb = multiclass_xg.predict(X_valid)\n",
    "    ovr_jaccard_score = jaccard_score(y_valid, valid_pred_xgb, average='samples') # TODO\n",
    "    ovr_ham_loss = metrics.hamming_loss(y_valid, valid_pred_xgb)\n",
    "    print(\"XGB-multi validation Jaccard score:\" , ovr_jaccard_score)\n",
    "    print(\"XGB-multi validation Hamming loss: \" , ovr_ham_loss)\n",
    "\n",
    "    Y_pred_ovr = multiclass_xg.predict(X_test)\n",
    "    a = jaccard_score(y_test, Y_pred_ovr, average='samples') #TODO\n",
    "    b = metrics.hamming_loss(y_test, Y_pred_ovr)\n",
    "    # print(Y_pred_ovr[800:805,:])\n",
    "    # print(y_test[800:805,:])\n",
    "\n",
    "    print(\"XGB-multi test Jaccard score: \", a)\n",
    "    print(\"XGB-multi test Hamming loss: \" , b)\n",
    "    chains = [ClassifierChain(base_xgb, order='random', random_state=i) for i in range(15)]\n",
    "    best_model_index = 0\n",
    "    best_jac = 0\n",
    "    for j, model in enumerate(chains):\n",
    "        model.fit(X_train, y_train)\n",
    "        valid_pred = model.predict(X_valid)\n",
    "        val_score =jaccard_score(y_valid, valid_pred, average='samples')\n",
    "        if val_score > best_jac:\n",
    "            best_model_index = j\n",
    "            best_jac = val_score\n",
    "        \n",
    "        \n",
    "    # predict on validation data\n",
    "    valid_pred_chains = chains[best_model_index].predict(X_valid)\n",
    "    chain_jaccard_scores = jaccard_score(y_valid, valid_pred_chains >= .5,\n",
    "                                    average='samples')\n",
    "                    \n",
    "    \n",
    "    print(\"CC Validation Jaccard Score:\\n \", chain_jaccard_scores)\n",
    "\n",
    "    chain_hamming_loss = metrics.hamming_loss(y_valid, valid_pred_chains >= .5)\n",
    "                \n",
    "    print(\"CC Validation Hamming Loss:\\n \", chain_hamming_loss)\n",
    "\n",
    "    # test on test data\n",
    "    Y_pred_chains = chains[best_model_index].predict(X_test)\n",
    "    chain_jaccard_scores = jaccard_score(y_test, Y_pred_chains >= .5,\n",
    "                                    average='samples')\n",
    "                    \n",
    "    frames_mean_jac_test.append(np.mean(chain_jaccard_scores))\n",
    "    print(\"CC Test Jaccard Score: \\n \", chain_jaccard_scores)\n",
    "\n",
    "    chain_hamming_loss = metrics.hamming_loss(y_test, Y_pred_chains) \n",
    "               \n",
    "    frames_mean_hm_test.append(chain_hamming_loss)\n",
    "    print(\"CC Test Hamming Loss:\\n \", chain_hamming_loss)\n",
    "    ## voting predicted labels\n",
    "    # test_result_df = pd.DataFrame(columns=metadata_cols, data=metadata_test.values)\n",
    "    # test_result_df.update(metadata_test)\n",
    "    temp_df = pd.DataFrame(data=Y_pred_chains, columns=label_cols)\n",
    "\n",
    "    # TODO: map emojis to emotions using temp_df\n",
    "\n",
    "    test_result_df = pd.concat([metadata_test, temp_df], axis=1)\n",
    "    video_groups = test_result_df.groupby('filename')[label_cols].sum()\n",
    "    ground_truth_video_labels = []\n",
    "    for v in video_groups.index.to_list():\n",
    "        # number of 1s in ground truth labels\n",
    "        ground_truth_video_labels.append(test_df[test_df['filename'] == v].iloc[0][label_cols])\n",
    "        \n",
    "        num_1s = test_df[test_df['filename'] == v].iloc[0][label_cols].sum()\n",
    "        num_1s = int(num_1s)\n",
    "        a = np.argsort(video_groups.loc[v].values)\n",
    "        for i in range(len(a) - 1, len(a) - num_1s - 1, -1):\n",
    "            video_groups.loc[v][a[i]] = 1\n",
    "        for i in range(0, len(a) - num_1s):\n",
    "            video_groups.loc[v][a[i]] = 0\n",
    "        # print(\"          &&&&&&&&&&&&&&&&            \")\n",
    "    print(np.array(ground_truth_video_labels,  dtype=int).shape)\n",
    "    print(video_groups.values.shape)\n",
    "\n",
    "    j = metrics.jaccard_score(np.array(ground_truth_video_labels,  dtype=int), video_groups.values, average='samples')\n",
    "    h = metrics.hamming_loss(np.array(ground_truth_video_labels,  dtype=int), video_groups.values)\n",
    "    videos_mean_jac_test.append(j)\n",
    "    videos_mean_hm_test.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['none', 'furious', 'anger', 'annoyed', 'contempt', 'disgust', 'hatred'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "emotion_mapped_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_mapped_df.at[np.random.randint(1, 100, size=(20, )), 'none'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "none         83\n",
       "furious     150\n",
       "anger        81\n",
       "annoyed     519\n",
       "contempt     28\n",
       "disgust     117\n",
       "hatred        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "emotion_mapped_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_mapped_df = pd.DataFrame(columns=emotion_cols, data=np.zeros((temp_df.shape[0], len(emotion_cols)), dtype=int))\n",
    "for emoji in label_cols.tolist():\n",
    "    indices = temp_df[temp_df[emoji]==1].index\n",
    "    emotions = na_emoji_map[emoji]\n",
    "    # print(indices)\n",
    "    # print(emotion)\n",
    "    for emotion in emotions:\n",
    "        emotion_mapped_df.at[indices.to_list(), emotion] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.15733333, 0.614     , 0.        , ..., 0.33124397, 0.50678979,\n",
       "        0.7027027 ],\n",
       "       [0.17066667, 0.602     , 0.        , ..., 0.33510125, 0.51656708,\n",
       "        0.6990504 ],\n",
       "       [0.10933333, 0.568     , 0.        , ..., 0.33751205, 0.51330798,\n",
       "        0.70197224],\n",
       "       ...,\n",
       "       [0.84533333, 0.442     , 0.1037464 , ..., 0.32594021, 0.46876697,\n",
       "        0.51424397],\n",
       "       [0.90666667, 0.484     , 0.0778098 , ..., 0.32594021, 0.47256926,\n",
       "        0.49963477],\n",
       "       [0.864     , 0.436     , 0.04034582, ..., 0.31918997, 0.4904943 ,\n",
       "        0.50255661]])"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "type(emotion_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      none  furious  anger  annoyed  contempt  disgust  hatred\n",
       "149    0.0      0.0    0.0      1.0       1.0      0.0     0.0\n",
       "150    0.0      0.0    0.0      1.0       1.0      0.0     0.0\n",
       "151    0.0      0.0    0.0      1.0       1.0      0.0     0.0\n",
       "152    0.0      0.0    0.0      1.0       1.0      0.0     0.0\n",
       "153    0.0      0.0    0.0      1.0       1.0      0.0     0.0\n",
       "...    ...      ...    ...      ...       ...      ...     ...\n",
       "6694   0.0      0.0    1.0      0.0       0.0      0.0     0.0\n",
       "6695   0.0      0.0    1.0      0.0       0.0      0.0     0.0\n",
       "6696   0.0      0.0    1.0      0.0       0.0      0.0     0.0\n",
       "6697   0.0      0.0    1.0      0.0       0.0      0.0     0.0\n",
       "6698   0.0      0.0    1.0      0.0       0.0      0.0     0.0\n",
       "\n",
       "[1590 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>none</th>\n      <th>furious</th>\n      <th>anger</th>\n      <th>annoyed</th>\n      <th>contempt</th>\n      <th>disgust</th>\n      <th>hatred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>149</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6694</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6695</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6696</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6697</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6698</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1590 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "y_test_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   none  furious  anger  annoyed  contempt  disgust  hatred\n",
       "0     0        0      0        0         0        0       0\n",
       "1     0        0      0        0         0        0       0\n",
       "2     0        0      0        0         0        0       0\n",
       "3     0        0      0        1         1        0       0\n",
       "4     0        0      0        0         0        0       0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>none</th>\n      <th>furious</th>\n      <th>anger</th>\n      <th>annoyed</th>\n      <th>contempt</th>\n      <th>disgust</th>\n      <th>hatred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "emotion_mapped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n        none       0.00      0.00      0.00        83\n     furious       0.09      0.03      0.05       150\n       anger       0.34      0.61      0.43       230\n     annoyed       0.59      0.66      0.62       595\n    contempt       0.21      0.10      0.13       425\n     disgust       0.19      0.91      0.31       117\n      hatred       0.00      0.00      0.00         0\n\n   micro avg       0.34      0.43      0.38      1600\n   macro avg       0.20      0.33      0.22      1600\nweighted avg       0.35      0.43      0.36      1600\n samples avg       0.35      0.27      0.29      1600\n\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(emotion_mapped_df.values,  y_test_emotions.values, target_names=emotion_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                precision    recall  f1-score   support\n\n       neutral       0.00      0.00      0.00         1\n         smirk       0.00      0.00      0.00         0\n       furious       0.00      0.00      0.00         1\n         weary       0.00      0.00      0.00         2\nexpressionless       0.00      0.00      0.00         6\n      unamused       0.80      0.67      0.73         6\n   rollingeyes       0.00      0.00      0.00         2\n          none       0.00      0.00      0.00         2\n     skeptical       0.00      0.00      0.00         0\n         angry       1.00      0.17      0.29         6\n     nauseated       0.50      0.33      0.40         3\n      vomiting       1.00      0.33      0.50         3\n       triumph       0.00      0.00      0.00         0\n        hatred       0.00      0.00      0.00         0\n\n     micro avg       0.22      0.22      0.22        32\n     macro avg       0.24      0.11      0.14        32\n  weighted avg       0.48      0.22      0.27        32\n   samples avg       0.12      0.12      0.12        32\n\n                precision    recall  f1-score   support\n\n       neutral       0.00      0.00      0.00        72\n         smirk       0.00      0.00      0.00         0\n       furious       0.03      0.09      0.05        54\n         weary       0.00      0.00      0.00       105\nexpressionless       0.32      0.12      0.17       537\n      unamused       0.73      0.46      0.57       621\n   rollingeyes       0.35      0.35      0.35       268\n          none       0.00      0.00      0.00        99\n     skeptical       0.00      0.00      0.00         0\n         angry       0.47      0.10      0.17       375\n     nauseated       0.31      0.15      0.20       193\n      vomiting       0.92      0.08      0.15       263\n       triumph       0.00      0.00      0.00         0\n        hatred       0.00      0.00      0.00         0\n\n     micro avg       0.41      0.21      0.27      2587\n     macro avg       0.22      0.10      0.12      2587\n  weighted avg       0.46      0.21      0.26      2587\n   samples avg       0.24      0.19      0.18      2587\n\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(np.array(ground_truth_video_labels,  dtype=int),  video_groups.values, target_names=label_cols.tolist()))\n",
    "print(metrics.classification_report(y_test,  Y_pred_chains, target_names=label_cols.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.grid(True)\n",
    "ax.set_title('CC Jaccard Score')\n",
    "ax.set_xticks(range(0,7))\n",
    "ax.set_xticklabels(range(0,7))\n",
    "ax.set_ylabel('Jaccard Similarity Score')\n",
    "ax.set_ylim([0.00, 1.0])\n",
    "\n",
    "plt.plot(frames_mean_jac_test, label='Frames Jaccard Score', color='blue')\n",
    "plt.plot(videos_mean_jac_test, label='Video Jaccard Score', color='green')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_mean_jac_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, Y_pred_chains, target_names=label_cols.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.grid(True)\n",
    "ax.set_title('CC Hamming Loss')\n",
    "ax.set_xticks(range(0,7))\n",
    "ax.set_xticklabels(range(0,7))\n",
    "ax.set_ylabel('Hamming Distance Loss')\n",
    "ax.set_ylim([0.00, 1.0])\n",
    "# ax.legend(('Frames Hamming Loss','Video Hamming Loss' ))\n",
    "plt.plot(frames_mean_hm_test, label='Frames Hamming Loss', color='red')\n",
    "plt.plot(videos_mean_hm_test, label='Video Hamming Loss', color='green')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_mean_jac_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "## MLTSVM is not compatible with later versions of numpy\n",
    "\n",
    "kfold = KFold(5, True, 1)\n",
    "frames_mean_hm_test = []\n",
    "frames_mean_jac_test = []\n",
    "videos_mean_jac_test = []\n",
    "videos_mean_hm_test = []\n",
    "col_indices = {i:label for (i,label) in enumerate(label_cols)}\n",
    "\n",
    "splits = kfold.split(train_videos)\n",
    "for (i, (train, test)) in enumerate(splits):\n",
    "    print('%d-th split: train: %d, validation: %d' % (i+1, len(videos[train]), len(videos[test])))\n",
    "    train_df = X_df[X_df['filename'].isin(train_videos[train])]\n",
    "    train_metadata = train_df[metadata_cols]\n",
    "    print('Training+validation data size: ', train_df.shape[0])\n",
    "    y_train = train_df[label_cols].values\n",
    "    X_train = train_df.drop(columns = ['frame', 'face_id', 'culture', 'filename', 'timestamp', 'confidence','success'] + label_cols.tolist()).values\n",
    "    valid_df = X_df[X_df['filename'].isin(train_videos[test])]\n",
    "    y_valid = valid_df[label_cols].values\n",
    "    X_valid = valid_df.drop(columns = ['frame', 'face_id', 'culture', 'filename', 'timestamp', 'confidence','success'] + label_cols.tolist()).values\n",
    "    \n",
    "    print('Training data size: ', X_train.shape[0])\n",
    "    print('Validation data size: ', X_valid.shape[0])\n",
    "    classifier = MLkNN(k=5)\n",
    "    # classifier = MLTSVM(c_k = 2**-1)\n",
    "    prediction = classifier.fit(X_train, y_train).predict(X_valid)\n",
    "\n",
    "    # Predicting on validation set\n",
    "    print(\"Validation Hamming Loss:\\n \", metrics.hamming_loss(y_valid, prediction))\n",
    "\n",
    "    # Predicting on test set\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    hm_loss = metrics.hamming_loss(y_test, y_test_pred)\n",
    "    frames_mean_hm_test.append(hm_loss)\n",
    "    print(\"Test Hamming Loss:\\n \", hm_loss)\n",
    "    jac_score = jaccard_score(y_test, y_test_pred,  average='samples')\n",
    "    frames_mean_jac_test.append(jac_score)\n",
    "    print(\"Test Jaccard Score:\\n \", jac_score)\n",
    "\n",
    "    # building test dataframe to vote labels\n",
    "    test_result_df = pd.DataFrame(columns=metadata_cols, data=metadata_test.values)\n",
    "    test_result_df.update(metadata_test)\n",
    "    # print(\"^^^^^^^^\", y_test_pred.toarray().shape)\n",
    "    temp_df = pd.DataFrame(data=y_test_pred.toarray(), columns=label_cols)\n",
    "    test_result_df = pd.concat([test_result_df, temp_df], axis=1)\n",
    "    \n",
    "    # print(test_result_df.head())\n",
    "    video_groups = test_result_df.groupby('filename')[label_cols].apply(lambda x : x.astype(int).sum())\n",
    "    # for name, group in video_groups:\n",
    "    #     print(name)\n",
    "    #     print(group)\n",
    "    #     print(\"\\n\") \n",
    "    ground_truth_video_labels = []\n",
    "    for v in video_groups.index.to_list():\n",
    "        # number of 1s in ground truth labels\n",
    "        ground_truth_video_labels.append(test_df[test_df['filename'] == v].iloc[0][label_cols])\n",
    "        print(len(ground_truth_video_labels))\n",
    "        num_1s = test_df[test_df['filename'] == v].iloc[0][label_cols].sum()\n",
    "        num_1s = int(num_1s)\n",
    "        a = np.argsort(video_groups.loc[v].values)\n",
    "        for i in range(len(a) - 1, len(a) - num_1s - 1, -1):\n",
    "            video_groups.loc[v][a[i]] = 1\n",
    "        for i in range(0, len(a) - num_1s):\n",
    "            video_groups.loc[v][a[i]] = 0\n",
    "\n",
    "    j = metrics.jaccard_score(np.array(ground_truth_video_labels,  dtype=int), video_groups.values, average='samples')\n",
    "    h = metrics.hamming_loss(np.array(ground_truth_video_labels,  dtype=int), video_groups.values)\n",
    "    videos_mean_jac_test.append(j)\n",
    "    videos_mean_hm_test.append(h)\n",
    "        "
   ]
  },
  {
   "source": [
    "Multilabel confusion matrix puts TN at (0,0) and TP at (1,1) position thanks @Kenneth Witham for pointing out.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(y_test, Y_pred_chains[1], labels=range(0,14))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, Y_pred_chains[-1], target_names=label_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.grid(True)\n",
    "ax.set_title('MLKNN Jaccard Score')\n",
    "ax.set_xticks(range(0,7))\n",
    "ax.set_xticklabels(range(0,7))\n",
    "ax.set_ylabel('Jaccard Similarity Score')\n",
    "ax.set_ylim([0.55, 1.0])\n",
    "\n",
    "plt.plot(frames_mean_jac_test, label='Frames Jaccard Score', color='blue')\n",
    "plt.plot(videos_mean_jac_test, label='Video Jaccard Score', color='green')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.grid(True)\n",
    "ax.set_title('MLKNN Hamming Loss')\n",
    "ax.set_xticks(range(0,7))\n",
    "ax.set_xticklabels(range(0,7))\n",
    "ax.set_ylabel('Hamming Distance Loss')\n",
    "ax.set_ylim([0.00, .1])\n",
    "# ax.legend(('Frames Hamming Loss','Video Hamming Loss' ))\n",
    "plt.plot(frames_mean_hm_test, label='Frames Hamming Loss', color='red')\n",
    "plt.plot(videos_mean_hm_test, label='Video Hamming Loss', color='green')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[800:805])\n",
    "print(metadata_test[800:805])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test))\n",
    "print(len(metadata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(input: str):\n",
    "    input = 'persian/' + input +\".mp4\"\n",
    "    return input\n",
    "X_df = pd.read_csv('../new_data/Persian/persian_dataset.csv', index_col=None)\n",
    "X_df['filename'] = X_df['filename'].apply(clean)\n",
    "X_df.to_csv('../new_data/Persian/persian_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}